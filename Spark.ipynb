{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0446e2e0",
   "metadata": {},
   "source": [
    "# Spark training from sparkbyexamples.com\n",
    "\n",
    "## PySpark Join Types\n",
    "\n",
    "### Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b37b0450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyspark\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "import pyspark\n",
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, MapType\n",
    "from pyspark.sql.functions import col, struct, when, lit, expr, sum, avg, max, min, mean, count \n",
    "from pyspark.sql.functions import udf, upper, transform, explode, map_keys, map_values\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.sql.functions import approx_count_distinct, avg, collect_list, collect_set, countDistinct, count, grouping, first, last\n",
    "from pyspark.sql.functions import kurtosis, max, min, mean, skewness, stddev, stddev_samp, stddev_pop, sum, sumDistinct\n",
    "from pyspark.sql.functions import variance, var_samp, var_pop, sum_distinct\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, percent_rank, ntile, cume_dist, lag, lead, col,avg,sum,min,max\n",
    "\n",
    "from pyspark.sql.types import MapType, StringType\n",
    "from pyspark.sql.functions import from_json, json_tuple, get_json_object, schema_of_json\n",
    "\n",
    "spark = SparkSession.builder.master('local[1]') \\\n",
    "    .appName('SparkByExamples.com') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f2cd3a",
   "metadata": {},
   "source": [
    "### Create base data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8bffc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- year_joined: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b9b4d6",
   "metadata": {},
   "source": [
    "### Inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a61057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id, 'inner').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe1667",
   "metadata": {},
   "source": [
    "### Full outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a8daeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, 'outer').show(truncate=False)\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, 'full').show(truncate=False)\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, 'fullouter').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065161eb",
   "metadata": {},
   "source": [
    "### Left outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd65ebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, 'left').show(truncate=False)\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, 'leftouter').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed75d8ea",
   "metadata": {},
   "source": [
    "### Right outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df7c77e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, 'right').show(truncate=False)\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, 'rightouter').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70b592",
   "metadata": {},
   "source": [
    " ### Left semi join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efe8ceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, 'leftsemi').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa59322",
   "metadata": {},
   "source": [
    "### Left anti join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbe85077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|6     |Brown|2              |2010       |50         |      |-1    |\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, 'leftanti').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741bf018",
   "metadata": {},
   "source": [
    "### Self join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8bf80d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------------+\n",
      "|emp_id|name    |superior_emp_id|superior_emp_name|\n",
      "+------+--------+---------------+-----------------+\n",
      "|2     |Rose    |1              |Smith            |\n",
      "|3     |Williams|1              |Smith            |\n",
      "|4     |Jones   |2              |Rose             |\n",
      "|5     |Brown   |2              |Rose             |\n",
      "|6     |Brown   |2              |Rose             |\n",
      "+------+--------+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.alias('emp1').join(empDF.alias('emp2'), \\\n",
    "        col('emp1.superior_emp_id') == col('emp2.emp_id'), 'inner') \\\n",
    "        .select(col('emp1.emp_id'), col('emp1.name'), \\\n",
    "               col('emp2.emp_id').alias('superior_emp_id'), \\\n",
    "               col('emp2.name').alias('superior_emp_name')) \\\n",
    "        .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36cadc4",
   "metadata": {},
   "source": [
    "### Using sql expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f01e033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.createOrReplaceTempView('EMP')\n",
    "deptDF.createOrReplaceTempView('DEPT')\n",
    "\n",
    "joinDF = spark.sql('select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id') \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "joinDF2 = spark.sql('select * from EMP e inner join DEPT d on e.emp_dept_id == d.dept_id') \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64584402",
   "metadata": {},
   "source": [
    "## PySpark union and union all\n",
    "\n",
    "### Create data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee66ea01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n",
    "  ]\n",
    "\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "simpleData2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "df2 = spark.createDataFrame(data = simpleData2, schema = columns2)\n",
    "\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7b59b1",
   "metadata": {},
   "source": [
    "### Merge two dataframes with union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87bdd9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "Count of merged dataframes: 9\n"
     ]
    }
   ],
   "source": [
    "unionDF = df.union(df2)\n",
    "unionDF.show(truncate=False)\n",
    "print('Count of merged dataframes: ' + str(unionDF.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4c7696",
   "metadata": {},
   "source": [
    "### Merge two dataframes using union all\n",
    "Union All has been deprecated since PySpark 2.0.0.  It is recommended that you use union instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "623502e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "Count of merged dataframes: 9\n"
     ]
    }
   ],
   "source": [
    "unionAllDF = df.unionAll(df2)\n",
    "unionAllDF.show(truncate=False)\n",
    "print('Count of merged dataframes: ' + str(unionAllDF.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8219aa",
   "metadata": {},
   "source": [
    "### Merge without duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b430b0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "Count of merged dataframes: 7\n"
     ]
    }
   ],
   "source": [
    "disDF = df.union(df2).distinct()\n",
    "disDF.show(truncate=False)\n",
    "print('Count of merged dataframes: ' + str(disDF.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626af696",
   "metadata": {},
   "source": [
    "## PySpark unionByName() \n",
    "### Build base data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90a52cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# Create DataFrame df1 with columns name, and id\n",
    "data = [(\"James\",34), (\"Michael\",56), \\\n",
    "        (\"Robert\",30), (\"Maria\",24) ]\n",
    "\n",
    "df1 = spark.createDataFrame(data = data, schema=[\"name\",\"id\"])\n",
    "df1.printSchema()\n",
    "\n",
    "# Create DataFrame df2 with columns name and id\n",
    "data2=[(34,\"James\"),(45,\"Maria\"), \\\n",
    "       (45,\"Jen\"),(34,\"Jeff\")]\n",
    "\n",
    "df2 = spark.createDataFrame(data = data2, schema = [\"id\",\"name\"])\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf91be2e",
   "metadata": {},
   "source": [
    "### unionByName() example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eab2c660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|name   |id |\n",
      "+-------+---+\n",
      "|James  |34 |\n",
      "|Michael|56 |\n",
      "|Robert |30 |\n",
      "|Maria  |24 |\n",
      "|James  |34 |\n",
      "|Maria  |45 |\n",
      "|Jen    |45 |\n",
      "|Jeff   |34 |\n",
      "+-------+---+\n",
      "\n",
      "Count of rows in new dataframe: 8\n"
     ]
    }
   ],
   "source": [
    "df3 = df1.unionByName(df2)\n",
    "df3.printSchema()\n",
    "df3.show(truncate=False)\n",
    "print('Count of rows in new dataframe: ' + str(df3.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eff0c1",
   "metadata": {},
   "source": [
    "### unionByName with different columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e76ed0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col0|col1|col2|col3|\n",
      "+----+----+----+----+\n",
      "|   5|   2|   6|null|\n",
      "|null|   6|   7|   3|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrames with different column names\n",
    "df1 = spark.createDataFrame([[5, 2, 6]], [\"col0\", \"col1\", \"col2\"])\n",
    "df2 = spark.createDataFrame([[6, 7, 3]], [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "# Using allowMissingColumns\n",
    "df3 = df1.unionByName(df2, allowMissingColumns=True)\n",
    "df3.printSchema\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fde9f74",
   "metadata": {},
   "source": [
    "## PySpark User Defined Function\n",
    "### Build base dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d39c6ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15ecb0",
   "metadata": {},
   "source": [
    "### Create user defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d267ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertCase(str):\n",
    "    resStr = ''\n",
    "    arr = str.split(' ')\n",
    "    for x in arr:\n",
    "        resStr = resStr + x[0:1].upper() + x[1:len(x)] + ' '\n",
    "        \n",
    "    return resStr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d232100",
   "metadata": {},
   "source": [
    "### Convert python function to UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da7f44f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "convertUDF = udf(lambda z: convertCase(z), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e236d73",
   "metadata": {},
   "source": [
    "### Use UDF with dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4f401c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here \n",
      "+-----+-------------+\n",
      "|Seqno|Name         |\n",
      "+-----+-------------+\n",
      "|1    |John Jones   |\n",
      "|2    |Tracey Smith |\n",
      "|3    |Amy Sanders  |\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(convertCase('here'))\n",
    "\n",
    "df.select(col('Seqno'), \\\n",
    "    convertUDF(col('Name')).alias('Name') ) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc91dc0",
   "metadata": {},
   "source": [
    "### Using UDF with PySpark dataframe withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54e3a4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------+\n",
      "|Seqno|Name        |Cureated Name|\n",
      "+-----+------------+-------------+\n",
      "|1    |john jones  |JOHN JONES   |\n",
      "|2    |tracey smith|TRACEY SMITH |\n",
      "|3    |amy sanders |AMY SANDERS  |\n",
      "+-----+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def upperCase(str):\n",
    "    return str.upper()\n",
    "\n",
    "upperCaseUDF = udf(lambda z: upperCase(z), StringType())\n",
    "\n",
    "df.withColumn('Cureated Name', upperCaseUDF(col('Name'))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f496d",
   "metadata": {},
   "source": [
    "### Registering PySpark UDF and use it on SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c102a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|Seqno|Name         |\n",
      "+-----+-------------+\n",
      "|1    |John Jones   |\n",
      "|2    |Tracey Smith |\n",
      "|3    |Amy Sanders  |\n",
      "+-----+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/26 20:57:53 WARN SimpleFunctionRegistry: The function convertudf replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register('convertUDF', convertCase, StringType())\n",
    "df.createOrReplaceTempView('NAME_TABLE')\n",
    "spark.sql('select Seqno, convertUDF(Name) as Name from NAME_TABLE') \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62357a20",
   "metadata": {},
   "source": [
    "### Creating UDF using annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23936b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------+\n",
      "|Seqno|Name        |Cureated Name|\n",
      "+-----+------------+-------------+\n",
      "|1    |john jones  |JOHN JONES   |\n",
      "|2    |tracey smith|TRACEY SMITH |\n",
      "|3    |amy sanders |AMY SANDERS  |\n",
      "+-----+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@udf(returnType=StringType())\n",
    "def upperCase(str):\n",
    "    return str.upper()\n",
    "\n",
    "df.withColumn('Cureated Name', upperCase(col('Name'))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9246c74",
   "metadata": {},
   "source": [
    "### Handling null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd7668ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "|4    |null        |\n",
      "+-----+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/26 21:02:54 ERROR Executor: Exception in task 0.0 in stage 122.0 (TID 92)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/k4/wb623tjn7rscpqqh2tbnc62r0000gn/T/ipykernel_1450/1659673845.py\", line 3, in convertCase\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1623)\n",
      "23/04/26 21:02:54 WARN TaskSetManager: Lost task 0.0 in stage 122.0 (TID 92) (ronalds-mbp.askey.com executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/k4/wb623tjn7rscpqqh2tbnc62r0000gn/T/ipykernel_1450/1659673845.py\", line 3, in convertCase\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1623)\n",
      "\n",
      "23/04/26 21:02:54 ERROR TaskSetManager: Task 0 in stage 122.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/k4/wb623tjn7rscpqqh2tbnc62r0000gn/T/ipykernel_1450/1659673845.py\", line 3, in convertCase\nAttributeError: 'NoneType' object has no attribute 'split'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m df2\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m df2\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNAME_TABLE2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mselect convertUDF(Name) from NAME_TABLE2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py:912\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    905\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    906\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m         },\n\u001b[1;32m    910\u001b[0m     )\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/k4/wb623tjn7rscpqqh2tbnc62r0000gn/T/ipykernel_1450/1659673845.py\", line 3, in convertCase\nAttributeError: 'NoneType' object has no attribute 'split'\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\"),\n",
    "    ('4',None)]\n",
    "\n",
    "df2 = spark.createDataFrame(data=data,schema=columns)\n",
    "df2.show(truncate=False)\n",
    "df2.createOrReplaceTempView(\"NAME_TABLE2\")\n",
    "\n",
    "spark.sql(\"select convertUDF(Name) from NAME_TABLE2\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ec7eb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|_nullsafeUDF(Name)|\n",
      "+------------------+\n",
      "|John Jones        |\n",
      "|Tracey Smith      |\n",
      "|Amy Sanders       |\n",
      "|                  |\n",
      "+------------------+\n",
      "\n",
      "+-----+-----------+\n",
      "|Seqno|Name       |\n",
      "+-----+-----------+\n",
      "|1    |John Jones |\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"_nullsafeUDF\", lambda str: convertCase(str) if not str is None else \"\" , StringType())\n",
    "\n",
    "spark.sql(\"select _nullsafeUDF(Name) from NAME_TABLE2\") \\\n",
    "     .show(truncate=False)\n",
    "\n",
    "spark.sql(\"select Seqno, _nullsafeUDF(Name) as Name from NAME_TABLE2 \" + \\\n",
    "          \" where Name is not null and _nullsafeUDF(Name) like '%John%'\") \\\n",
    "     .show(truncate=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73956358",
   "metadata": {},
   "source": [
    "## PySpark transform() function\n",
    "\n",
    "### Create base data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5011de5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CourseName: string (nullable = true)\n",
      " |-- fee: long (nullable = true)\n",
      " |-- discount: long (nullable = true)\n",
      "\n",
      "+----------+----+--------+\n",
      "|CourseName|fee |discount|\n",
      "+----------+----+--------+\n",
      "|Java      |4000|5       |\n",
      "|Python    |4600|10      |\n",
      "|Scala     |4500|15      |\n",
      "|Scala     |4100|15      |\n",
      "|PHP       |3000|20      |\n",
      "+----------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = (('Java',4000,5), \\\n",
    "             ('Python',4600,10), \\\n",
    "             ('Scala',4500,15), \\\n",
    "              ('Scala',4100,15), \\\n",
    "             ('PHP',3000,20), \\\n",
    "             )\n",
    "\n",
    "columns = ['CourseName','fee','discount']\n",
    "\n",
    "#Create data frame\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ac183",
   "metadata": {},
   "source": [
    "### Create custom functions/transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "92546a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create first custom transformation\n",
    "def to_upper_str_columns(df):\n",
    "    return df.withColumn('CourseName',upper(df.CourseName))\n",
    "\n",
    "#Create second custom transformation\n",
    "def reduce_price(reduceBy):\n",
    "    def inner(df):\n",
    "        return df.withColumn('new_fee', df.fee - reduceBy)\n",
    "    return inner\n",
    "\n",
    "#Create third custom transformation\n",
    "def apply_discount(df):\n",
    "    return df.withColumn('discounted_fee', \\\n",
    "                        df.new_fee - (df.new_fee * df.discount) / 100)\n",
    "\n",
    "#Create fourth transformation\n",
    "def select_columns(df):\n",
    "    return df.select('CourseName','discounted_fee')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db5e0a6",
   "metadata": {},
   "source": [
    "### PySpark apply dataframe transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "be6e9ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CourseName: string (nullable = true)\n",
      " |-- fee: long (nullable = true)\n",
      " |-- discount: long (nullable = true)\n",
      " |-- new_fee: long (nullable = true)\n",
      " |-- discounted_fee: double (nullable = true)\n",
      "\n",
      "+----------+----+--------+-------+--------------+\n",
      "|CourseName|fee |discount|new_fee|discounted_fee|\n",
      "+----------+----+--------+-------+--------------+\n",
      "|JAVA      |4000|5       |3000   |2850.0        |\n",
      "|PYTHON    |4600|10      |3600   |3240.0        |\n",
      "|SCALA     |4500|15      |3500   |2975.0        |\n",
      "|SCALA     |4100|15      |3100   |2635.0        |\n",
      "|PHP       |3000|20      |2000   |1600.0        |\n",
      "+----------+----+--------+-------+--------------+\n",
      "\n",
      "root\n",
      " |-- CourseName: string (nullable = true)\n",
      " |-- discounted_fee: double (nullable = true)\n",
      "\n",
      "+----------+--------------+\n",
      "|CourseName|discounted_fee|\n",
      "+----------+--------------+\n",
      "|JAVA      |2850.0        |\n",
      "|PYTHON    |3240.0        |\n",
      "|SCALA     |2975.0        |\n",
      "|SCALA     |2635.0        |\n",
      "|PHP       |1600.0        |\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df2 = df.transform(to_upper_str_columns) \\\n",
    "        #.transform(reduce_price,1000) \\\n",
    "#.transform(apply_discount)\n",
    "#df2 = df.transform(reduce_price,1000)\n",
    "df2 = df.transform(to_upper_str_columns) \\\n",
    "    .transform(reduce_price(1000)) \\\n",
    "    .transform(apply_discount) \n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "df2 = df.transform(to_upper_str_columns) \\\n",
    "    .transform(reduce_price(1000)) \\\n",
    "    .transform(apply_discount) \\\n",
    "    .transform(select_columns)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0522ea94",
   "metadata": {},
   "source": [
    "## PySpark sql.functions.transform\n",
    "\n",
    "### Create base data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bb9263d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Languages1: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Languages2: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+----------------+------------------+---------------+\n",
      "|            Name|        Languages1|     Languages2|\n",
      "+----------------+------------------+---------------+\n",
      "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|\n",
      "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|\n",
      "+----------------+------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]),\n",
    " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]),\n",
    " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"])\n",
    "]\n",
    "df = spark.createDataFrame(data=data,schema=[\"Name\",\"Languages1\",\"Languages2\"])\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350c223c",
   "metadata": {},
   "source": [
    "### Use transform() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "06c2035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|languages1        |\n",
      "+------------------+\n",
      "|[JAVA, SCALA, C++]|\n",
      "|[SPARK, JAVA, C++]|\n",
      "|[CSHARP, VB]      |\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(transform('Languages1', lambda x: upper(x)).alias('languages1')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30961310",
   "metadata": {},
   "source": [
    "## PySpark appl function to column\n",
    "\n",
    "Create base data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5d64323a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ec11ce",
   "metadata": {},
   "source": [
    "### apply function using withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2c1d6284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+\n",
      "|Seqno|Name        |Upper_Name  |\n",
      "+-----+------------+------------+\n",
      "|1    |john jones  |JOHN JONES  |\n",
      "|2    |tracey smith|TRACEY SMITH|\n",
      "|3    |amy sanders |AMY SANDERS |\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('Upper_Name', upper(df.Name)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2973329c",
   "metadata": {},
   "source": [
    "### apply function using select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "28b9068d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+\n",
      "|Seqno|        Name|  Upper_Name|\n",
      "+-----+------------+------------+\n",
      "|    1|  john jones|  JOHN JONES|\n",
      "|    2|tracey smith|TRACEY SMITH|\n",
      "|    3| amy sanders| AMY SANDERS|\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Seqno','Name',upper(df.Name).alias('Upper_Name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9bc4366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+\n",
      "|Seqno|        Name|  Upper_Name|\n",
      "+-----+------------+------------+\n",
      "|    1|  john jones|  JOHN JONES|\n",
      "|    2|tracey smith|TRACEY SMITH|\n",
      "|    3| amy sanders| AMY SANDERS|\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('TAB')\n",
    "spark.sql('select Seqno, Name, upper(Name) as Upper_Name from TAB').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b840110e",
   "metadata": {},
   "source": [
    "### Create custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f020b1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upperCase(str):\n",
    "    return str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd19b59c",
   "metadata": {},
   "source": [
    "### Register UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5195e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "upperCaseUDF = udf(lambda x:upperCase(x), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f3532c",
   "metadata": {},
   "source": [
    "### Apply custom UDF to column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4a81e72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------+\n",
      "|Seqno|Name        |Cureated Name|\n",
      "+-----+------------+-------------+\n",
      "|1    |john jones  |JOHN JONES   |\n",
      "|2    |tracey smith|TRACEY SMITH |\n",
      "|3    |amy sanders |AMY SANDERS  |\n",
      "+-----+------------+-------------+\n",
      "\n",
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |JOHN JONES  |\n",
      "|2    |TRACEY SMITH|\n",
      "|3    |AMY SANDERS |\n",
      "+-----+------------+\n",
      "\n",
      "+-----+------------+------------+\n",
      "|Seqno|Name        |Upper_Name  |\n",
      "+-----+------------+------------+\n",
      "|1    |john jones  |JOHN JONES  |\n",
      "|2    |tracey smith|TRACEY SMITH|\n",
      "|3    |amy sanders |AMY SANDERS |\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Custom UDF using withColumn\n",
    "df.withColumn('Cureated Name', upperCaseUDF(col('Name'))).show(truncate=False)\n",
    "\n",
    "# Custom UDF with select()\n",
    "df.select(col('Seqno'), \\\n",
    "         upperCaseUDF(col('Name')).alias('Name')).show(truncate=False)\n",
    "\n",
    "# Custom UDF with sql\n",
    "spark.udf.register('upperCaseUDF', upperCaseUDF)\n",
    "df.createOrReplaceTempView('TAB')\n",
    "spark.sql('select Seqno, Name, upperCaseUDF(Name) as Upper_Name from TAB').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1623f1fd",
   "metadata": {},
   "source": [
    "### PySpark Pandas apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e46fc6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\pyspark\\pandas\\internal.py:1536: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  fields = [\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Fee  Discount\n",
      "0  20000.0      1000\n",
      "1  25000.0      2500\n",
      "2  30000.0      1500\n",
      "3  22000.0      1200\n",
      "4      NaN      3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\pyspark\\pandas\\internal.py:1536: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  fields = [\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    21000.0\n",
      "1    27500.0\n",
      "2    31500.0\n",
      "3    23200.0\n",
      "4        NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "technologies = ({\n",
    "    'Fee' :[20000,25000,30000,22000,np.NaN],\n",
    "    'Discount':[1000,2500,1500,1200,3000]\n",
    "               })\n",
    "\n",
    "# Create data frame\n",
    "psdf = ps.DataFrame(technologies)\n",
    "print(psdf)\n",
    "\n",
    "def add(data):\n",
    "    return data[0] + data[1]\n",
    "\n",
    "addDF = psdf.apply(add, axis=1)\n",
    "print(addDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d68f80",
   "metadata": {},
   "source": [
    "## PySpark map() transformations\n",
    "\n",
    "Create base data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b51e198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"Project\",\"Gutenbergs\",\"Alices\",\"Adventures\",\n",
    "\"in\",\"Wonderland\",\"Project\",\"Gutenbergs\",\"Adventures\",\n",
    "\"in\",\"Wonderland\",\"Project\",\"Gutenbergs\"]\n",
    "\n",
    "rdd=spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a6b61c",
   "metadata": {},
   "source": [
    "### PySpark map() example with RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a9d8f23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Project', 1)\n",
      "('Gutenbergs', 1)\n",
      "('Alices', 1)\n",
      "('Adventures', 1)\n",
      "('in', 1)\n",
      "('Wonderland', 1)\n",
      "('Project', 1)\n",
      "('Gutenbergs', 1)\n",
      "('Adventures', 1)\n",
      "('in', 1)\n",
      "('Wonderland', 1)\n",
      "('Project', 1)\n",
      "('Gutenbergs', 1)\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd.map(lambda x: (x,1))\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ba804",
   "metadata": {},
   "source": [
    "### PySpark map() example with data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "68b8184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n",
      "|firstname|lastname|gender|salary|\n",
      "+---------+--------+------+------+\n",
      "|    James|   Smith|     M|    30|\n",
      "|     Anna|    Rose|     F|    41|\n",
      "|   Robert|Williams|     M|    62|\n",
      "+---------+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James','Smith','M',30),\n",
    "       ('Anna','Rose','F',41),\n",
    "       ('Robert','Williams','M',62),\n",
    "       ]\n",
    "\n",
    "columns = ['firstname','lastname','gender','salary']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e2140896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+----------+\n",
      "|            name|gender|new_salary|\n",
      "+----------------+------+----------+\n",
      "|    Smith, James|     M|        60|\n",
      "|      Rose, Anna|     F|        82|\n",
      "|Williams, Robert|     M|       124|\n",
      "+----------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Refering to columns by index\n",
    "rdd2 = df.rdd.map(lambda x: (x[1] + ', '+ x[0],x[2],x[3]*2))\n",
    "df2=rdd2.toDF(['name','gender','new_salary'])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0c379bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+------+\n",
      "|            name|gender|salary|\n",
      "+----------------+------+------+\n",
      "|    Smith, James|     M|    60|\n",
      "|      Rose, Anna|     F|    82|\n",
      "|Williams, Robert|     M|   124|\n",
      "+----------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Refering to column names\n",
    "rdd2=df.rdd.map(lambda x: (x['lastname'] + ', ' + x['firstname'], x['gender'], x['salary'] * 2))\n",
    "df3 = rdd2.toDF(['name','gender','salary'])\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "791554f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+------+\n",
      "|            name|gender|salary|\n",
      "+----------------+------+------+\n",
      "|    Smith, James|     M|    60|\n",
      "|      Rose, Anna|     F|    82|\n",
      "|Williams, Robert|     M|   124|\n",
      "+----------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Also refering to column names\n",
    "rdd2 = df.rdd.map(lambda x: (x.lastname+', '+x.firstname, x.gender, x.salary*2))\n",
    "df4 = rdd2.toDF(['name','gender','salary'])\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2fda83a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+------+\n",
      "|            name|gender|salary|\n",
      "+----------------+------+------+\n",
      "|    Smith, James|     m|    60|\n",
      "|      Rose, Anna|     f|    82|\n",
      "|Williams, Robert|     m|   124|\n",
      "+----------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# By calling function\n",
    "def func1(x):\n",
    "    firstName=x.firstname\n",
    "    lastName=x.lastname\n",
    "    name=lastName + ', ' + firstName\n",
    "    gender=x.gender.lower()\n",
    "    salary=x.salary * 2\n",
    "    \n",
    "    return(name, gender, salary)\n",
    "\n",
    "rdd2 = df.rdd.map(lambda x: func1(x))\n",
    "df5 = rdd2.toDF(['name','gender','salary'])\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f76fbd5",
   "metadata": {},
   "source": [
    "## PySpark flatMap() transformation\n",
    "\n",
    "### Create base data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c26277da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenbergs\n",
      "Alices Adventures in Wonderland\n",
      "Project Gutenbergs\n",
      "Adventures in Wonderland\n",
      "Project Gutenbergs\n"
     ]
    }
   ],
   "source": [
    "data = [\"Project Gutenbergs\",\n",
    "        \"Alices Adventures in Wonderland\",\n",
    "        \"Project Gutenbergs\",\n",
    "        \"Adventures in Wonderland\",\n",
    "        \"Project Gutenbergs\"]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "for element in rdd.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe5e5e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project\n",
      "Gutenbergs\n",
      "Alices\n",
      "Adventures\n",
      "in\n",
      "Wonderland\n",
      "Project\n",
      "Gutenbergs\n",
      "Adventures\n",
      "in\n",
      "Wonderland\n",
      "Project\n",
      "Gutenbergs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd2=rdd.flatMap(lambda x: x.split(' '))\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a686f683",
   "metadata": {},
   "source": [
    "### Using flatMap() transformation on dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c013b9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- col: string (nullable = true)\n",
      "\n",
      "+---------+------+\n",
      "|name     |col   |\n",
      "+---------+------+\n",
      "|James    |Java  |\n",
      "|James    |Scala |\n",
      "|Michael  |Spark |\n",
      "|Michael  |Java  |\n",
      "|Michael  |null  |\n",
      "|Robert   |CSharp|\n",
      "|Robert   |      |\n",
      "|Jefferson|1     |\n",
      "|Jefferson|2     |\n",
      "|Ron      |Python|\n",
      "|Ron      |Scala |\n",
      "|Ron      |SQL   |\n",
      "|Ron      |VB    |\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{}), \n",
    "        ('Ron',['Python','Scala','SQL','VB'],{'hair':'black','eye':'brown'})]\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
    "\n",
    "df2 = df.select(df.name, explode(df.knownLanguages))\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfdc602",
   "metadata": {},
   "source": [
    "## PySpark DataFrame foreach()\n",
    "\n",
    "### Create base data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4625420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|        Name|\n",
      "+-----+------------+\n",
      "|    1|  john jones|\n",
      "|    2|tracey smith|\n",
      "|    3| amy sanders|\n",
      "+-----+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#create data\n",
    "columns = ['Seqno','Name']\n",
    "\n",
    "data = [('1','john jones'),\n",
    "       ('2','tracey smith'),\n",
    "       ('3','amy sanders')]\n",
    "\n",
    "#create data frame\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()\n",
    "\n",
    "#foreach example\n",
    "def f(df):\n",
    "    print(df.Seqno)\n",
    "    \n",
    "df.foreach(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2848a3",
   "metadata": {},
   "source": [
    "### foreach with accumulator example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcb62a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "accum = spark.sparkContext.accumulator(0)\n",
    "df.foreach(lambda x:accum.add(int(x.Seqno)))\n",
    "print(accum.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa59aa8",
   "metadata": {},
   "source": [
    "### PySpark RDD foreach() usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54901bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "accum = spark.sparkContext.accumulator(0)\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd.foreach(lambda x:accum.add(x))\n",
    "print(accum.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c04b0ac",
   "metadata": {},
   "source": [
    "## PySpark Random Sample\n",
    "\n",
    "### Using fraction to get a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b5431c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=3), Row(id=20), Row(id=25), Row(id=54), Row(id=65), Row(id=76), Row(id=84), Row(id=98)]\n"
     ]
    }
   ],
   "source": [
    "# Create data frame\n",
    "df = spark.range(100)\n",
    "print(df.sample(0.06).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5979df86",
   "metadata": {},
   "source": [
    "### Using seed to reproduce the same samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8001f16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=36), Row(id=37), Row(id=41), Row(id=43), Row(id=56), Row(id=66), Row(id=69), Row(id=75), Row(id=83)]\n",
      "[Row(id=36), Row(id=37), Row(id=41), Row(id=43), Row(id=56), Row(id=66), Row(id=69), Row(id=75), Row(id=83)]\n",
      "[Row(id=19), Row(id=21), Row(id=42), Row(id=48), Row(id=49), Row(id=50), Row(id=75), Row(id=80)]\n"
     ]
    }
   ],
   "source": [
    "print(df.sample(0.1,123).collect())\n",
    "\n",
    "print(df.sample(0.1,123).collect())\n",
    "\n",
    "print(df.sample(0.1,456).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca5de28",
   "metadata": {},
   "source": [
    "### Stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "034655bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(key=1), Row(key=1), Row(key=0), Row(key=0), Row(key=1), Row(key=0), Row(key=1), Row(key=1), Row(key=0), Row(key=0), Row(key=1)]\n"
     ]
    }
   ],
   "source": [
    "df2 = df.select((df.id % 3).alias('key'))\n",
    "print(df2.sampleBy('key', {0: 0.1, 1: 0.2}).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac50779",
   "metadata": {},
   "source": [
    "## PySpark fillna() and fill() - Replace null/none values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8730816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- zipcode: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      "\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|id |zipcode|type    |city               |state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|1  |704    |STANDARD|null               |PR   |30100     |\n",
      "|2  |704    |null    |PASEO COSTA DEL SUR|PR   |null      |\n",
      "|3  |709    |null    |BDA SAN LUIS       |PR   |3700      |\n",
      "|4  |76166  |UNIQUE  |CINGULAR WIRELESS  |TX   |84000     |\n",
      "|5  |76177  |STANDARD|null               |TX   |null      |\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filePath = 'resources/small_zipcode.csv'\n",
    "df = spark.read.options(header='true', inferSchema='true').csv(filePath)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd4121b",
   "metadata": {},
   "source": [
    "### PySpark replace null/none values with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf3ed07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               null|   PR|     30100|\n",
      "|  2|    704|    null|PASEO COSTA DEL SUR|   PR|         0|\n",
      "|  3|    709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|               null|   TX|         0|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               null|   PR|     30100|\n",
      "|  2|    704|    null|PASEO COSTA DEL SUR|   PR|         0|\n",
      "|  3|    709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|               null|   TX|         0|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Replace 0 for null for all integer columns\n",
    "df.na.fill(value=0).show()\n",
    "#Replace 0 for null on only population column\n",
    "df.na.fill(value=0, subset=['population']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9765a55b",
   "metadata": {},
   "source": [
    "### PySpark replace null/none with empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49173b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|id |zipcode|type    |city               |state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|1  |704    |STANDARD|                   |PR   |30100     |\n",
      "|2  |704    |        |PASEO COSTA DEL SUR|PR   |null      |\n",
      "|3  |709    |        |BDA SAN LUIS       |PR   |3700      |\n",
      "|4  |76166  |UNIQUE  |CINGULAR WIRELESS  |TX   |84000     |\n",
      "|5  |76177  |STANDARD|                   |TX   |null      |\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ab94968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|id |zipcode|type    |city               |state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|1  |704    |STANDARD|unknown            |PR   |30100     |\n",
      "|2  |704    |        |PASEO COSTA DEL SUR|PR   |null      |\n",
      "|3  |709    |        |BDA SAN LUIS       |PR   |3700      |\n",
      "|4  |76166  |UNIQUE  |CINGULAR WIRELESS  |TX   |84000     |\n",
      "|5  |76177  |STANDARD|unknown            |TX   |null      |\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('unknown', ['city']).na.fill('',['type']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d090837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|id |zipcode|type    |city               |state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|1  |704    |STANDARD|unknown            |PR   |30100     |\n",
      "|2  |704    |        |PASEO COSTA DEL SUR|PR   |null      |\n",
      "|3  |709    |        |BDA SAN LUIS       |PR   |3700      |\n",
      "|4  |76166  |UNIQUE  |CINGULAR WIRELESS  |TX   |84000     |\n",
      "|5  |76177  |STANDARD|unknown            |TX   |null      |\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill({'city':'unknown', 'type':''}).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0fcead",
   "metadata": {},
   "source": [
    "## PySpark Pivot and Unpivot DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86a4be54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Amount: long (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "|Banana |1000  |USA    |\n",
      "|Carrots|1500  |USA    |\n",
      "|Beans  |1600  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Banana |400   |China  |\n",
      "|Carrots|1200  |China  |\n",
      "|Beans  |1500  |China  |\n",
      "|Orange |4000  |China  |\n",
      "|Banana |2000  |Canada |\n",
      "|Carrots|2000  |Canada |\n",
      "|Beans  |2000  |Mexico |\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
    "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n",
    "\n",
    "columns= [\"Product\",\"Amount\",\"Country\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa1ba5",
   "metadata": {},
   "source": [
    "### Pivot PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f97df201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Canada: long (nullable = true)\n",
      " |-- China: long (nullable = true)\n",
      " |-- Mexico: long (nullable = true)\n",
      " |-- USA: long (nullable = true)\n",
      "\n",
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico|USA |\n",
      "+-------+------+-----+------+----+\n",
      "|Orange |null  |4000 |null  |4000|\n",
      "|Beans  |null  |1500 |2000  |1600|\n",
      "|Banana |2000  |400  |null  |1000|\n",
      "|Carrots|2000  |1200 |null  |1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivotDF = df.groupby('Product').pivot('Country').sum('Amount')\n",
    "pivotDF.printSchema()\n",
    "pivotDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa3e7559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Amount: long (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "|Banana |1000  |USA    |\n",
      "|Carrots|1500  |USA    |\n",
      "|Beans  |1600  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Banana |400   |China  |\n",
      "|Carrots|1200  |China  |\n",
      "|Beans  |1500  |China  |\n",
      "|Orange |4000  |China  |\n",
      "|Banana |2000  |Canada |\n",
      "|Carrots|2000  |Canada |\n",
      "|Beans  |2000  |Mexico |\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries = ['USA','China','Canada','Mexico']\n",
    "pivotDF = df.groupby('Product').pivot('Country', countries).sum('Amount')\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa991d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico|USA |\n",
      "+-------+------+-----+------+----+\n",
      "|Orange |null  |4000 |null  |4000|\n",
      "|Beans  |null  |1500 |2000  |1600|\n",
      "|Banana |2000  |400  |null  |1000|\n",
      "|Carrots|2000  |1200 |null  |1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivotDF = df.groupby('Product','Country') \\\n",
    "    .sum('Amount') \\\n",
    "    .groupby('Product') \\\n",
    "    .pivot('Country') \\\n",
    "    .sum('sum(Amount)') \n",
    "\n",
    "pivotDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c4171",
   "metadata": {},
   "source": [
    "### Unpivot PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4e27f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Product|Country|Total|\n",
      "+-------+-------+-----+\n",
      "|Orange |China  |4000 |\n",
      "|Beans  |China  |1500 |\n",
      "|Beans  |Mexico |2000 |\n",
      "|Banana |Canada |2000 |\n",
      "|Banana |China  |400  |\n",
      "|Carrots|Canada |2000 |\n",
      "|Carrots|China  |1200 |\n",
      "+-------+-------+-----+\n",
      "\n",
      "+-------+-------+-----+\n",
      "|Product|Country|Total|\n",
      "+-------+-------+-----+\n",
      "| Orange|  China| 4000|\n",
      "|  Beans|  China| 1500|\n",
      "|  Beans| Mexico| 2000|\n",
      "| Banana| Canada| 2000|\n",
      "| Banana|  China|  400|\n",
      "|Carrots| Canada| 2000|\n",
      "|Carrots|  China| 1200|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unpivotExpr = \"stack(3, 'Canada', Canada, 'China', China, 'Mexico', Mexico) as (Country,Total)\"\n",
    "unPivotDF = pivotDF.select(\"Product\", expr(unpivotExpr)) \\\n",
    "    .where(\"Total is not null\")\n",
    "unPivotDF.show(truncate=False)\n",
    "unPivotDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3713e32",
   "metadata": {},
   "source": [
    "## PySpark partitionBy() - Write to disk example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ed0b2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      "\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "|RecordNumber|Country|City               |Zipcode|State|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "|1           |US     |PARC PARQUE        |704    |PR   |\n",
      "|2           |US     |PASEO COSTA DEL SUR|704    |PR   |\n",
      "|10          |US     |BDA SAN LUIS       |709    |PR   |\n",
      "|49347       |US     |HOLT               |32564  |FL   |\n",
      "|49348       |US     |HOMOSASSA          |34487  |FL   |\n",
      "|61391       |US     |CINGULAR WIRELESS  |76166  |TX   |\n",
      "|61392       |US     |FORT WORTH         |76177  |TX   |\n",
      "|61393       |US     |FT WORTH           |76177  |TX   |\n",
      "|54356       |US     |SPRUCE PINE        |35585  |AL   |\n",
      "|76511       |US     |ASH HILL           |27007  |NC   |\n",
      "|4           |US     |URB EUGENE RICE    |704    |PR   |\n",
      "|39827       |US     |MESA               |85209  |AZ   |\n",
      "|39828       |US     |MESA               |85210  |AZ   |\n",
      "|49345       |US     |HILLIARD           |32046  |FL   |\n",
      "|49346       |US     |HOLDER             |34445  |FL   |\n",
      "|3           |US     |SECT LANAUSSE      |704    |PR   |\n",
      "|54354       |US     |SPRING GARDEN      |36275  |AL   |\n",
      "|54355       |US     |SPRINGVILLE        |35146  |AL   |\n",
      "|76512       |US     |ASHEBORO           |27203  |NC   |\n",
      "|76513       |US     |ASHEBORO           |27204  |NC   |\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('header',True).csv('resources/simple-zipcodes.csv')\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f99bc",
   "metadata": {},
   "source": [
    "### PySpark partitionBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0d58e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option('header',True) \\\n",
    "    .partitionBy('state') \\\n",
    "    .mode('overwrite') \\\n",
    "    .csv('resource/zipcodes-state')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db49774",
   "metadata": {},
   "source": [
    "### PySpark partitionBy() multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "878efb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option('header',True) \\\n",
    "    .partitionBy('state','city') \\\n",
    "    .mode('overwrite') \\\n",
    "    .csv('resource/zipcodes-state')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d76843",
   "metadata": {},
   "source": [
    "### Using repartition() and partitionBy() together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b5aedaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(2) \\\n",
    "    .write.option('header',True) \\\n",
    "    .partitionBy('state') \\\n",
    "    .mode('overwrite') \\\n",
    "    .csv('resource/zipcodes-state-more')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed27c445",
   "metadata": {},
   "source": [
    "### Data Skew - Control number of records per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d74fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option('header',True) \\\n",
    "  .option('maxRecordsPerFile',2) \\\n",
    "  .partitionBy('state') \\\n",
    "  .mode('overwrite') \\\n",
    "  .csv('resource/tmp/zipcodes-state')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d8264",
   "metadata": {},
   "source": [
    "### Read a specific partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe9c0dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      "\n",
      "+------------+-------+-------+\n",
      "|RecordNumber|Country|Zipcode|\n",
      "+------------+-------+-------+\n",
      "|54355       |US     |35146  |\n",
      "+------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSinglePart = spark.read.option('header',True) \\\n",
    "  .csv('resource/zipcodes-state/state=AL/city=SPRINGVILLE')\n",
    "dfSinglePart.printSchema()\n",
    "dfSinglePart.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7efe86",
   "metadata": {},
   "source": [
    "### PySpark SQL - Read Partition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da2a63cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------+-----+-----------+\n",
      "|RecordNumber|Country|Zipcode|state|city       |\n",
      "+------------+-------+-------+-----+-----------+\n",
      "|54355       |US     |35146  |AL   |SPRINGVILLE|\n",
      "+------------+-------+-------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parqDF = spark.read.option('header',True) \\\n",
    "  .csv('resource/zipcodes-state')\n",
    "\n",
    "parqDF.createOrReplaceTempView('ZIPCODE')\n",
    "spark.sql(\"select * from ZIPCODE where state = 'AL' and city = 'SPRINGVILLE'\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46a0c50",
   "metadata": {},
   "source": [
    "## PySpark MapType (Dict) Usage with Examples\n",
    "\n",
    "### Create PySpark Map Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2eedc44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapType(StringType(), StringType(), False)\n"
     ]
    }
   ],
   "source": [
    "mapCol = MapType(StringType(), StringType(),False)\n",
    "print(mapCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea849f5",
   "metadata": {},
   "source": [
    "### Create MapType from StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48b91594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('name', StringType(), True), StructField('properties', MapType(StringType(), StringType(), True), True)])\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('properties', MapType(StringType(), StringType()),True)\n",
    "])\n",
    "\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2999f55",
   "metadata": {},
   "source": [
    "### Create DataFrame using StructType schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0266346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+----------+-----------------------------+\n",
      "|name      |properties                   |\n",
      "+----------+-----------------------------+\n",
      "|James     |{eye -> brown, hair -> black}|\n",
      "|Michael   |{eye -> null, hair -> brown} |\n",
      "|Robert    |{eye -> black, hair -> red}  |\n",
      "|Washington|{eye -> grey, hair -> grey}  |\n",
      "|Jefferson |{eye -> , hair -> brown}     |\n",
      "+----------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDictionary = [\n",
    "    ('James',{'hair':'black','eye':'brown'}),\n",
    "    ('Michael', {'hair':'brown', 'eye':None}),\n",
    "    ('Robert', {'hair':'red', 'eye':'black'}),\n",
    "    ('Washington', {'hair':'grey', 'eye':'grey'}),\n",
    "    ('Jefferson', {'hair':'brown', 'eye':''})\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=dataDictionary,schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec1a62b",
   "metadata": {},
   "source": [
    "### Access PySpark MapType Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccc1bd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- hair: string (nullable = true)\n",
      " |-- eye: string (nullable = true)\n",
      "\n",
      "+----------+-----+-----+\n",
      "|      name| hair|  eye|\n",
      "+----------+-----+-----+\n",
      "|     James|black|brown|\n",
      "|   Michael|brown| null|\n",
      "|    Robert|  red|black|\n",
      "|Washington| grey| grey|\n",
      "| Jefferson|brown|     |\n",
      "+----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3=df.rdd.map(lambda x: \\\n",
    "    (x.name,x.properties[\"hair\"],x.properties[\"eye\"])) \\\n",
    "    .toDF([\"name\",\"hair\",\"eye\"])\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4703800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+\n",
      "|name      |hair |eye  |\n",
      "+----------+-----+-----+\n",
      "|James     |black|brown|\n",
      "|Michael   |brown|null |\n",
      "|Robert    |red  |black|\n",
      "|Washington|grey |grey |\n",
      "|Jefferson |brown|     |\n",
      "+----------+-----+-----+\n",
      "\n",
      "+----------+-----+-----+\n",
      "|name      |hair |eye  |\n",
      "+----------+-----+-----+\n",
      "|James     |black|brown|\n",
      "|Michael   |brown|null |\n",
      "|Robert    |red  |black|\n",
      "|Washington|grey |grey |\n",
      "|Jefferson |brown|     |\n",
      "+----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('hair', df.properties.getItem('hair')) \\\n",
    "  .withColumn('eye', df.properties.getItem('eye')) \\\n",
    "  .drop('properties') \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "df.withColumn('hair',df.properties['hair']) \\\n",
    "  .withColumn('eye',df.properties['eye']) \\\n",
    "  .drop('properties') \\\n",
    "  .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04eea94",
   "metadata": {},
   "source": [
    "### Functions - explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14be196f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+\n",
      "|name      |key |value|\n",
      "+----------+----+-----+\n",
      "|James     |eye |brown|\n",
      "|James     |hair|black|\n",
      "|Michael   |eye |null |\n",
      "|Michael   |hair|brown|\n",
      "|Robert    |eye |black|\n",
      "|Robert    |hair|red  |\n",
      "|Washington|eye |grey |\n",
      "|Washington|hair|grey |\n",
      "|Jefferson |eye |     |\n",
      "|Jefferson |hair|brown|\n",
      "+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name,explode(df.properties)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2709216",
   "metadata": {},
   "source": [
    "### Functions - map_keys() - Get All Map Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4130dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|name      |map_keys(properties)|\n",
      "+----------+--------------------+\n",
      "|James     |[eye, hair]         |\n",
      "|Michael   |[eye, hair]         |\n",
      "|Robert    |[eye, hair]         |\n",
      "|Washington|[eye, hair]         |\n",
      "|Jefferson |[eye, hair]         |\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_keys\n",
    "df.select(df.name,map_keys(df.properties)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe872469",
   "metadata": {},
   "source": [
    "### Functions - map_values() - Get All Map Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd0bcc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+\n",
      "|name      |map_values(properties)|\n",
      "+----------+----------------------+\n",
      "|James     |[brown, black]        |\n",
      "|Michael   |[null, brown]         |\n",
      "|Robert    |[black, red]          |\n",
      "|Washington|[grey, grey]          |\n",
      "|Jefferson |[, brown]             |\n",
      "+----------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_values\n",
    "df.select(df.name, map_values(df.properties)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f89c25",
   "metadata": {},
   "source": [
    "# PySpark SQL Functions\n",
    "\n",
    "## PySpark Aggregate Functions\n",
    "\n",
    "### Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c281da45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88237e0c",
   "metadata": {},
   "source": [
    "### approx_count_distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90b0a327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approx_count_distinct: 6\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct, avg, collect_list, collect_set, countDistinct, count, grouping, first, last\n",
    "from pyspark.sql.functions import kurtosis, max, min, mean, skewness, stddev, stddev_samp, stddev_pop, sum, sumDistinct\n",
    "from pyspark.sql.functions import variance, var_samp, var_pop\n",
    "\n",
    "print(\"approx_count_distinct: \" + \\\n",
    "      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fb3ada",
   "metadata": {},
   "source": [
    "### avg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f94b6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg: 3400.0\n"
     ]
    }
   ],
   "source": [
    "print('avg: ' + str(df.select(avg('salary')).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba86a97",
   "metadata": {},
   "source": [
    "### collect_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ba3fe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|collect_list(salary)                                        |\n",
      "+------------------------------------------------------------+\n",
      "|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n",
      "+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(collect_list('salary')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8601ce",
   "metadata": {},
   "source": [
    "### collect_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8015d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|collect_set(salary)                 |\n",
      "+------------------------------------+\n",
      "|[2000, 3300, 4100, 3000, 4600, 3900]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(collect_set('salary')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23791fb",
   "metadata": {},
   "source": [
    "### countDistinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05276bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|count(DISTINCT department, salary)|\n",
      "+----------------------------------+\n",
      "|8                                 |\n",
      "+----------------------------------+\n",
      "\n",
      "Distinct count of department & salary: 8\n"
     ]
    }
   ],
   "source": [
    "df2 = df.select(countDistinct('department','salary'))\n",
    "df2.show(truncate=False)\n",
    "print('Distinct count of department & salary: ' + str(df2.collect()[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386c8134",
   "metadata": {},
   "source": [
    "### first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48111d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|first(salary)|\n",
      "+-------------+\n",
      "|3000         |\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(first('salary')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eaaa02",
   "metadata": {},
   "source": [
    "### last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0b03ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|last(salary)|\n",
      "+------------+\n",
      "|4100        |\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(last('salary')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c2af0",
   "metadata": {},
   "source": [
    "### kurtosis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56d1b62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|kurtosis(salary)   |\n",
      "+-------------------+\n",
      "|-0.6467803030303032|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(kurtosis('salary')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97877a20",
   "metadata": {},
   "source": [
    "### max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2871eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|max(salary)|\n",
      "+-----------+\n",
      "|4600       |\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(max('salary')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74d7e2",
   "metadata": {},
   "source": [
    "### min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50f6e41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(salary)|\n",
      "+-----------+\n",
      "|2000       |\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(min('salary')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210a242c",
   "metadata": {},
   "source": [
    "### mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26fd82b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|3400.0     |\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(mean('salary')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089ea719",
   "metadata": {},
   "source": [
    "### skewness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "624c3f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|skewness(salary)    |\n",
      "+--------------------+\n",
      "|-0.12041791181069571|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(skewness('salary')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bce58d",
   "metadata": {},
   "source": [
    "### stddev(), stddev_samp(), and stddev_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec1e9586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+\n",
      "|stddev_samp(salary)|stddev_samp(salary)|stddev_pop(salary)|\n",
      "+-------------------+-------------------+------------------+\n",
      "|765.9416862050705  |765.9416862050705  |726.636084983398  |\n",
      "+-------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(stddev('salary'), stddev_samp('salary'), stddev_pop('salary')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37df172",
   "metadata": {},
   "source": [
    "### sum(), and sum_distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b602824a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|sum(salary)|sum(DISTINCT salary)|\n",
      "+-----------+--------------------+\n",
      "|34000      |20900               |\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(sum('salary'), sum_distinct('salary')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fb5e41",
   "metadata": {},
   "source": [
    "### variance(), var_samp(), and var_pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60f059c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+---------------+\n",
      "|var_samp(salary) |var_samp(salary) |var_pop(salary)|\n",
      "+-----------------+-----------------+---------------+\n",
      "|586666.6666666666|586666.6666666666|528000.0       |\n",
      "+-----------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(variance('salary'), var_samp('salary'), var_pop('salary')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f78e2d",
   "metadata": {},
   "source": [
    "## PySpark Window Functions\n",
    "\n",
    "### Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22bb411b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8660ada",
   "metadata": {},
   "source": [
    "### row_number() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9c5e3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "|James        |Sales     |3000  |1         |\n",
      "|James        |Sales     |3000  |2         |\n",
      "|Robert       |Sales     |4100  |3         |\n",
      "|Saif         |Sales     |4100  |4         |\n",
      "|Michael      |Sales     |4600  |5         |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy('department').orderBy('salary')\n",
    "\n",
    "df.withColumn('row_number', row_number().over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e4c6cd",
   "metadata": {},
   "source": [
    "### rank() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3185989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|Maria        |Finance   |3000  |1   |\n",
      "|Scott        |Finance   |3300  |2   |\n",
      "|Jen          |Finance   |3900  |3   |\n",
      "|Kumar        |Marketing |2000  |1   |\n",
      "|Jeff         |Marketing |3000  |2   |\n",
      "|James        |Sales     |3000  |1   |\n",
      "|James        |Sales     |3000  |1   |\n",
      "|Robert       |Sales     |4100  |3   |\n",
      "|Saif         |Sales     |4100  |3   |\n",
      "|Michael      |Sales     |4600  |5   |\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('rank', rank().over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15ea4b6",
   "metadata": {},
   "source": [
    "### dense_rank() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e68c2af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|dense_rank|\n",
      "+-------------+----------+------+----------+\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "|James        |Sales     |3000  |1         |\n",
      "|James        |Sales     |3000  |1         |\n",
      "|Robert       |Sales     |4100  |2         |\n",
      "|Saif         |Sales     |4100  |2         |\n",
      "|Michael      |Sales     |4600  |3         |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('dense_rank', dense_rank().over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f60a37",
   "metadata": {},
   "source": [
    "### percent_rank() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c260ff1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------+\n",
      "|employee_name|department|salary|percent_rank|\n",
      "+-------------+----------+------+------------+\n",
      "|Maria        |Finance   |3000  |0.0         |\n",
      "|Scott        |Finance   |3300  |0.5         |\n",
      "|Jen          |Finance   |3900  |1.0         |\n",
      "|Kumar        |Marketing |2000  |0.0         |\n",
      "|Jeff         |Marketing |3000  |1.0         |\n",
      "|James        |Sales     |3000  |0.0         |\n",
      "|James        |Sales     |3000  |0.0         |\n",
      "|Robert       |Sales     |4100  |0.5         |\n",
      "|Saif         |Sales     |4100  |0.5         |\n",
      "|Michael      |Sales     |4600  |1.0         |\n",
      "+-------------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('percent_rank',percent_rank().over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c756f997",
   "metadata": {},
   "source": [
    "### ntile() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf3cdb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|Maria        |Finance   |3000  |1    |\n",
      "|Scott        |Finance   |3300  |1    |\n",
      "|Jen          |Finance   |3900  |2    |\n",
      "|Kumar        |Marketing |2000  |1    |\n",
      "|Jeff         |Marketing |3000  |2    |\n",
      "|James        |Sales     |3000  |1    |\n",
      "|James        |Sales     |3000  |1    |\n",
      "|Robert       |Sales     |4100  |1    |\n",
      "|Saif         |Sales     |4100  |2    |\n",
      "|Michael      |Sales     |4600  |2    |\n",
      "+-------------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('ntile', ntile(2).over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c0b8b0",
   "metadata": {},
   "source": [
    "### cume_dist() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58bdd0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------------+\n",
      "|employee_name|department|salary|cume_dist         |\n",
      "+-------------+----------+------+------------------+\n",
      "|Maria        |Finance   |3000  |0.3333333333333333|\n",
      "|Scott        |Finance   |3300  |0.6666666666666666|\n",
      "|Jen          |Finance   |3900  |1.0               |\n",
      "|Kumar        |Marketing |2000  |0.5               |\n",
      "|Jeff         |Marketing |3000  |1.0               |\n",
      "|James        |Sales     |3000  |0.4               |\n",
      "|James        |Sales     |3000  |0.4               |\n",
      "|Robert       |Sales     |4100  |0.8               |\n",
      "|Saif         |Sales     |4100  |0.8               |\n",
      "|Michael      |Sales     |4600  |1.0               |\n",
      "+-------------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('cume_dist',cume_dist().over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce5804c",
   "metadata": {},
   "source": [
    "### lag() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "12612562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|lag |\n",
      "+-------------+----------+------+----+\n",
      "|Maria        |Finance   |3000  |null|\n",
      "|Scott        |Finance   |3300  |null|\n",
      "|Jen          |Finance   |3900  |3000|\n",
      "|Kumar        |Marketing |2000  |null|\n",
      "|Jeff         |Marketing |3000  |null|\n",
      "|James        |Sales     |3000  |null|\n",
      "|James        |Sales     |3000  |null|\n",
      "|Robert       |Sales     |4100  |3000|\n",
      "|Saif         |Sales     |4100  |3000|\n",
      "|Michael      |Sales     |4600  |4100|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('lag',lag('salary',2).over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16154a2d",
   "metadata": {},
   "source": [
    "### lead() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1868813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|lead|\n",
      "+-------------+----------+------+----+\n",
      "|Maria        |Finance   |3000  |3900|\n",
      "|Scott        |Finance   |3300  |null|\n",
      "|Jen          |Finance   |3900  |null|\n",
      "|Kumar        |Marketing |2000  |null|\n",
      "|Jeff         |Marketing |3000  |null|\n",
      "|James        |Sales     |3000  |4100|\n",
      "|James        |Sales     |3000  |4100|\n",
      "|Robert       |Sales     |4100  |4600|\n",
      "|Saif         |Sales     |4100  |null|\n",
      "|Michael      |Sales     |4600  |null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('lead',lead('salary',2).over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d13c86e",
   "metadata": {},
   "source": [
    "### Window aggregrate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcfccce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+----+\n",
      "|department|avg   |sum  |min |max |\n",
      "+----------+------+-----+----+----+\n",
      "|Finance   |3400.0|10200|3000|3900|\n",
      "|Marketing |2500.0|5000 |2000|3000|\n",
      "|Sales     |3760.0|18800|3000|4600|\n",
      "+----------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpecAgg = Window.partitionBy('department')\n",
    "\n",
    "df.withColumn('row',row_number().over(windowSpec)) \\\n",
    "  .withColumn('avg',avg(col('salary')).over(windowSpecAgg)) \\\n",
    "  .withColumn('sum',sum(col('salary')).over(windowSpecAgg)) \\\n",
    "  .withColumn('min',min(col('salary')).over(windowSpecAgg)) \\\n",
    "  .withColumn('max',max(col('salary')).over(windowSpecAgg)) \\\n",
    "  .where(col('row')==1).select('department','avg','sum','min','max') \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52deeff9",
   "metadata": {},
   "source": [
    "## PySpark SQL Date & Timestamp Functions\n",
    "\n",
    "### Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "02910002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e9b8802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|     input|\n",
      "+---+----------+\n",
      "|  1|2020-02-01|\n",
      "|  2|2019-03-01|\n",
      "|  3|2021-03-01|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
    "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572224ef",
   "metadata": {},
   "source": [
    "### current_date() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f74fc542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|current_date|\n",
      "+------------+\n",
      "|  2023-05-03|\n",
      "+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date().alias('current_date')).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86fb3c7",
   "metadata": {},
   "source": [
    "### date_format() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "080a8136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|input     |date_format|\n",
      "+----------+-----------+\n",
      "|2020-02-01|02-01-2020 |\n",
      "|2019-03-01|03-01-2019 |\n",
      "|2021-03-01|03-01-2021 |\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('input'), date_format(col('input'), 'MM-dd-yyyy').alias('date_format')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84d4e82",
   "metadata": {},
   "source": [
    "### to_date() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b72add62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|input     |to_date   |\n",
      "+----------+----------+\n",
      "|2020-02-01|2020-02-01|\n",
      "|2019-03-01|2019-03-01|\n",
      "|2021-03-01|2021-03-01|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('input'), to_date(col('input'), 'yyyy-MM-dd').alias('to_date')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f89a5ce",
   "metadata": {},
   "source": [
    "### datediff() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2765a56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|input     |datediff|\n",
      "+----------+--------+\n",
      "|2020-02-01|1187    |\n",
      "|2019-03-01|1524    |\n",
      "|2021-03-01|793     |\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('input'), datediff(current_date(), col('input')).alias('datediff')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e48739",
   "metadata": {},
   "source": [
    "### months_between() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7bffc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|input     |months_between|\n",
      "+----------+--------------+\n",
      "|2020-02-01|39.06451613   |\n",
      "|2019-03-01|50.06451613   |\n",
      "|2021-03-01|26.06451613   |\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('input'), months_between(current_date(), col('input')).alias('months_between')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc89d7f",
   "metadata": {},
   "source": [
    "### trunc() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d501ab18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+-----------+\n",
      "|input     |Month_trunc|Year_trunc|Month_trunc|\n",
      "+----------+-----------+----------+-----------+\n",
      "|2020-02-01|2020-02-01 |2020-01-01|2020-02-01 |\n",
      "|2019-03-01|2019-03-01 |2019-01-01|2019-03-01 |\n",
      "|2021-03-01|2021-03-01 |2021-01-01|2021-03-01 |\n",
      "+----------+-----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('input'), \n",
    "         trunc(col('input'), 'Month').alias('Month_trunc'),\n",
    "         trunc(col('input'), 'Year').alias('Year_trunc'), \n",
    "         trunc(col('input'), 'Month').alias('Month_trunc')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9581a29b",
   "metadata": {},
   "source": [
    "### add_months(), date_add(), and date_sub() functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bc9fed57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+\n",
      "|     input|add_months|sub_months|  date_add|  date_sub|\n",
      "+----------+----------+----------+----------+----------+\n",
      "|2020-02-01|2020-05-01|2019-11-01|2020-02-05|2020-01-28|\n",
      "|2019-03-01|2019-06-01|2018-12-01|2019-03-05|2019-02-25|\n",
      "|2021-03-01|2021-06-01|2020-12-01|2021-03-05|2021-02-25|\n",
      "+----------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"input\"), \n",
    "    add_months(col(\"input\"),3).alias(\"add_months\"), \n",
    "    add_months(col(\"input\"),-3).alias(\"sub_months\"), \n",
    "    date_add(col(\"input\"),4).alias(\"date_add\"), \n",
    "    date_sub(col(\"input\"),4).alias(\"date_sub\") \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67f2376",
   "metadata": {},
   "source": [
    "### year(), month(), next_day(), and weekofyear() functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9d2c83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+----------+----------+\n",
      "|     input|year|month|  next_day|weekofyear|\n",
      "+----------+----+-----+----------+----------+\n",
      "|2020-02-01|2020|    2|2020-02-02|         5|\n",
      "|2019-03-01|2019|    3|2019-03-03|         9|\n",
      "|2021-03-01|2021|    3|2021-03-07|         9|\n",
      "+----------+----+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"input\"), \n",
    "     year(col(\"input\")).alias(\"year\"), \n",
    "     month(col(\"input\")).alias(\"month\"), \n",
    "     next_day(col(\"input\"),\"Sunday\").alias(\"next_day\"), \n",
    "     weekofyear(col(\"input\")).alias(\"weekofyear\") \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6699f",
   "metadata": {},
   "source": [
    "### dayofweek(), dayofmonth(), and dayofyear() functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fca151b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+---------+\n",
      "|     input|dayofweek|dayofmonth|dayofyear|\n",
      "+----------+---------+----------+---------+\n",
      "|2020-02-01|        7|         1|       32|\n",
      "|2019-03-01|        6|         1|       60|\n",
      "|2021-03-01|        2|         1|       60|\n",
      "+----------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"input\"),  \n",
    "     dayofweek(col(\"input\")).alias(\"dayofweek\"), \n",
    "     dayofmonth(col(\"input\")).alias(\"dayofmonth\"), \n",
    "     dayofyear(col(\"input\")).alias(\"dayofyear\"), \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe750f47",
   "metadata": {},
   "source": [
    "### current_timestamp() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2299c65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|id |input                  |\n",
      "+---+-----------------------+\n",
      "|1  |02-01-2020 11 01 19 06 |\n",
      "|2  |03-01-2019 12 01 19 406|\n",
      "|3  |03-01-2021 12 01 19 406|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\n",
    "df2=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2298cc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|current_timestamp))       |\n",
      "+--------------------------+\n",
      "|2023-05-03 14:39:33.547658|\n",
      "+--------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(current_timestamp().alias('current_timestamp))')).show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef1d8ee",
   "metadata": {},
   "source": [
    "### to_timestamp() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "764ce324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------------+\n",
      "|input                  |to_timestamp           |\n",
      "+-----------------------+-----------------------+\n",
      "|02-01-2020 11 01 19 06 |2020-02-01 11:01:19.06 |\n",
      "|03-01-2019 12 01 19 406|2019-03-01 12:01:19.406|\n",
      "|03-01-2021 12 01 19 406|2021-03-01 12:01:19.406|\n",
      "+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(col('input'), \n",
    "          to_timestamp(col('input'), 'MM-dd-yyyy HH mm ss SSS').alias('to_timestamp')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3cd663",
   "metadata": {},
   "source": [
    "### hour(), Minute(), and second() functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a7858da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+------+------+\n",
      "|input                  |hour|minute|second|\n",
      "+-----------------------+----+------+------+\n",
      "|2020-02-01 11:01:19.06 |11  |1     |19    |\n",
      "|2019-03-01 12:01:19.406|12  |1     |19    |\n",
      "|2021-03-01 12:01:19.406|12  |1     |19    |\n",
      "+-----------------------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\n",
    "df3=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "\n",
    "df3.select(col(\"input\"), \n",
    "    hour(col(\"input\")).alias(\"hour\"), \n",
    "    minute(col(\"input\")).alias(\"minute\"),\n",
    "    second(col(\"input\")).alias(\"second\") \n",
    "  ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df2a1d",
   "metadata": {},
   "source": [
    "## PySpark JSON Functions\n",
    "\n",
    "### Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "940ee13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------------------------+\n",
      "|id |value                                                                     |\n",
      "+---+--------------------------------------------------------------------------+\n",
      "|1  |{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
      "+---+--------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonString=\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"\n",
    "df=spark.createDataFrame([(1, jsonString)],[\"id\",\"value\"])\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e4612",
   "metadata": {},
   "source": [
    "### from_json() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "99a16e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- value: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+---+---------------------------------------------------------------------------+\n",
      "|id |value                                                                      |\n",
      "+---+---------------------------------------------------------------------------+\n",
      "|1  |{Zipcode -> 704, ZipCodeType -> STANDARD, City -> PARC PARQUE, State -> PR}|\n",
      "+---+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=df.withColumn(\"value\",from_json(df.value,MapType(StringType(),StringType())))\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a4d39b",
   "metadata": {},
   "source": [
    "### to_json() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6f7f946a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------------------------+\n",
      "|id |value                                                                       |\n",
      "+---+----------------------------------------------------------------------------+\n",
      "|1  |{\"Zipcode\":\"704\",\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
      "+---+----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumn('value', to_json(col('value'))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022cc7ad",
   "metadata": {},
   "source": [
    "### json_tuple() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1f8a63fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+-----------+\n",
      "|id |Zipcode|ZipCodeType|City       |\n",
      "+---+-------+-----------+-----------+\n",
      "|1  |704    |STANDARD   |PARC PARQUE|\n",
      "+---+-------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('id'), json_tuple(col('value'), 'Zipcode','ZipCodeType','City')) \\\n",
    "  .toDF('id','Zipcode','ZipCodeType','City') \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50796b2",
   "metadata": {},
   "source": [
    "### get_json_object() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f2674089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|id |ZipCodeType|\n",
      "+---+-----------+\n",
      "|1  |STANDARD   |\n",
      "+---+-----------+\n",
      "\n",
      "+---+-----------+\n",
      "|id |ZipCodeType|\n",
      "+---+-----------+\n",
      "|1  |STANDARD   |\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.show(truncate=False)\n",
    "df.select(col('id'),get_json_object(col('value'),'$.ZipCodeType').alias('ZipCodeType')).show(truncate=False)\n",
    "\n",
    "df.select(col(\"id\"),get_json_object(col(\"value\"),\"$.ZipCodeType\").alias(\"ZipCodeType\")) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c970b3",
   "metadata": {},
   "source": [
    "### schema_of_json() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "edeffca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRUCT<City: STRING, State: STRING, ZipCodeType: STRING, Zipcode: BIGINT>\n"
     ]
    }
   ],
   "source": [
    "schemaStr=spark.range(1) \\\n",
    "    .select(schema_of_json(lit(\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"))) \\\n",
    "    .collect()[0][0]\n",
    "print(schemaStr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
