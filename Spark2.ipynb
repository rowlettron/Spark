{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28df215b",
   "metadata": {},
   "source": [
    "# Spark training from sparkbyexamples.com\n",
    "\n",
    "## PySpark Data Sources\n",
    "\n",
    "### PySpark Read&Write CSV file\n",
    "\n",
    "### Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9de5b1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ron/opt/anaconda3/lib/python3.9/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/05 19:51:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/05 19:51:19 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyspark\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "import pyspark\n",
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, MapType, DoubleType, BooleanType\n",
    "from pyspark.sql.functions import col, struct, when, lit, expr, sum, avg, max, min, mean, count \n",
    "from pyspark.sql.functions import udf, upper, transform, explode, map_keys, map_values\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.sql.functions import approx_count_distinct, avg, collect_list, collect_set, countDistinct, count, grouping, first, last\n",
    "from pyspark.sql.functions import kurtosis, max, min, mean, skewness, stddev, stddev_samp, stddev_pop, sum, sumDistinct\n",
    "from pyspark.sql.functions import variance, var_samp, var_pop, sum_distinct\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, percent_rank, ntile, cume_dist, lag, lead, col,avg,sum,min,max\n",
    "\n",
    "from pyspark.sql.types import MapType, StringType\n",
    "from pyspark.sql.functions import from_json, json_tuple, get_json_object, schema_of_json\n",
    "\n",
    "spark = SparkSession.builder.master('local[1]') \\\n",
    "    .appName('SparkByExamples.com') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fc277e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('resources/zipcodes.csv')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "444075ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using a fully qualified data source name\n",
    "df = spark.read.format('csv').load('resources/zipcodes.csv')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6a00ba",
   "metadata": {},
   "source": [
    "### Using header record for column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b077ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Lat: string (nullable = true)\n",
      " |-- Long: string (nullable = true)\n",
      " |-- Xaxis: string (nullable = true)\n",
      " |-- Yaxis: string (nullable = true)\n",
      " |-- Zaxis: string (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Decommisioned: string (nullable = true)\n",
      " |-- TaxReturnsFiled: string (nullable = true)\n",
      " |-- EstimatedPopulation: string (nullable = true)\n",
      " |-- TotalWages: string (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.option('header',True).csv('resources/zipcodes.csv')\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea597ab",
   "metadata": {},
   "source": [
    "### Options while reading csv file\n",
    "### delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ab98697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: integer (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Xaxis: double (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- TaxReturnsFiled: integer (nullable = true)\n",
      " |-- EstimatedPopulation: integer (nullable = true)\n",
      " |-- TotalWages: integer (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.read.options(header=True,delimiter=',',inferSchema=True) \\\n",
    " .csv('resources/zipcodes.csv')\n",
    "df3.printSchema()\n",
    "#df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701dc03c",
   "metadata": {},
   "source": [
    "### Read csv file with a user-specified custom schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56c0ff91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: integer (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Xaxis: integer (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- TaxReturnsFiled: string (nullable = true)\n",
      " |-- EstimatedPopulation: integer (nullable = true)\n",
      " |-- TotalWages: integer (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      "\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+-----------------------+----------------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|RecordNumber|Zipcode|ZipCodeType|City               |State|LocationType  |Lat  |Long   |Xaxis|Yaxis|Zaxis|WorldRegion|Country|LocationText           |Location                    |Decommisioned|TaxReturnsFiled|EstimatedPopulation|TotalWages|Notes        |\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+-----------------------+----------------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|1           |704    |STANDARD   |PARC PARQUE        |PR   |NOT ACCEPTABLE|17.96|-66.22 |null |-0.87|0.3  |NA         |US     |Parc Parque, PR        |NA-US-PR-PARC PARQUE        |false        |null           |null               |null      |null         |\n",
      "|2           |704    |STANDARD   |PASEO COSTA DEL SUR|PR   |NOT ACCEPTABLE|17.96|-66.22 |null |-0.87|0.3  |NA         |US     |Paseo Costa Del Sur, PR|NA-US-PR-PASEO COSTA DEL SUR|false        |null           |null               |null      |null         |\n",
      "|10          |709    |STANDARD   |BDA SAN LUIS       |PR   |NOT ACCEPTABLE|18.14|-66.26 |null |-0.86|0.31 |NA         |US     |Bda San Luis, PR       |NA-US-PR-BDA SAN LUIS       |false        |null           |null               |null      |null         |\n",
      "|61391       |76166  |UNIQUE     |CINGULAR WIRELESS  |TX   |NOT ACCEPTABLE|32.72|-97.31 |null |-0.83|0.54 |NA         |US     |Cingular Wireless, TX  |NA-US-TX-CINGULAR WIRELESS  |false        |null           |null               |null      |null         |\n",
      "|61392       |76177  |STANDARD   |FORT WORTH         |TX   |PRIMARY       |32.75|-97.33 |null |-0.83|0.54 |NA         |US     |Fort Worth, TX         |NA-US-TX-FORT WORTH         |false        |2126           |4053               |122396986 |null         |\n",
      "|61393       |76177  |STANDARD   |FT WORTH           |TX   |ACCEPTABLE    |32.75|-97.33 |null |-0.83|0.54 |NA         |US     |Ft Worth, TX           |NA-US-TX-FT WORTH           |false        |2126           |4053               |122396986 |null         |\n",
      "|4           |704    |STANDARD   |URB EUGENE RICE    |PR   |NOT ACCEPTABLE|17.96|-66.22 |null |-0.87|0.3  |NA         |US     |Urb Eugene Rice, PR    |NA-US-PR-URB EUGENE RICE    |false        |null           |null               |null      |null         |\n",
      "|39827       |85209  |STANDARD   |MESA               |AZ   |PRIMARY       |33.37|-111.64|null |-0.77|0.55 |NA         |US     |Mesa, AZ               |NA-US-AZ-MESA               |false        |14962          |26883              |563792730 |no NWS data, |\n",
      "|39828       |85210  |STANDARD   |MESA               |AZ   |PRIMARY       |33.38|-111.84|null |-0.77|0.55 |NA         |US     |Mesa, AZ               |NA-US-AZ-MESA               |false        |14374          |25446              |471000465 |null         |\n",
      "|49345       |32046  |STANDARD   |HILLIARD           |FL   |PRIMARY       |30.69|-81.92 |null |-0.85|0.51 |NA         |US     |Hilliard, FL           |NA-US-FL-HILLIARD           |false        |3922           |7443               |133112149 |null         |\n",
      "|49346       |34445  |PO BOX     |HOLDER             |FL   |PRIMARY       |28.96|-82.41 |null |-0.86|0.48 |NA         |US     |Holder, FL             |NA-US-FL-HOLDER             |false        |null           |null               |null      |null         |\n",
      "|49347       |32564  |STANDARD   |HOLT               |FL   |PRIMARY       |30.72|-86.67 |null |-0.85|0.51 |NA         |US     |Holt, FL               |NA-US-FL-HOLT               |false        |1207           |2190               |36395913  |null         |\n",
      "|49348       |34487  |PO BOX     |HOMOSASSA          |FL   |PRIMARY       |28.78|-82.61 |null |-0.86|0.48 |NA         |US     |Homosassa, FL          |NA-US-FL-HOMOSASSA          |false        |null           |null               |null      |null         |\n",
      "|10          |708    |STANDARD   |BDA SAN LUIS       |PR   |NOT ACCEPTABLE|18.14|-66.26 |null |-0.86|0.31 |NA         |US     |Bda San Luis, PR       |NA-US-PR-BDA SAN LUIS       |false        |null           |null               |null      |null         |\n",
      "|3           |704    |STANDARD   |SECT LANAUSSE      |PR   |NOT ACCEPTABLE|17.96|-66.22 |null |-0.87|0.3  |NA         |US     |Sect Lanausse, PR      |NA-US-PR-SECT LANAUSSE      |false        |null           |null               |null      |null         |\n",
      "|54354       |36275  |PO BOX     |SPRING GARDEN      |AL   |PRIMARY       |33.97|-85.55 |null |-0.82|0.55 |NA         |US     |Spring Garden, AL      |NA-US-AL-SPRING GARDEN      |false        |null           |null               |null      |null         |\n",
      "|54355       |35146  |STANDARD   |SPRINGVILLE        |AL   |PRIMARY       |33.77|-86.47 |null |-0.82|0.55 |NA         |US     |Springville, AL        |NA-US-AL-SPRINGVILLE        |false        |4046           |7845               |172127599 |null         |\n",
      "|54356       |35585  |STANDARD   |SPRUCE PINE        |AL   |PRIMARY       |34.37|-87.69 |null |-0.82|0.56 |NA         |US     |Spruce Pine, AL        |NA-US-AL-SPRUCE PINE        |false        |610            |1209               |18525517  |null         |\n",
      "|76511       |27007  |STANDARD   |ASH HILL           |NC   |NOT ACCEPTABLE|36.4 |-80.56 |null |-0.79|0.59 |NA         |US     |Ash Hill, NC           |NA-US-NC-ASH HILL           |false        |842            |1666               |28876493  |null         |\n",
      "|76512       |27203  |STANDARD   |ASHEBORO           |NC   |PRIMARY       |35.71|-79.81 |null |-0.79|0.58 |NA         |US     |Asheboro, NC           |NA-US-NC-ASHEBORO           |false        |8355           |15228              |215474318 |null         |\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+-----------------------+----------------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType() \\\n",
    "      .add(\"RecordNumber\",IntegerType(),True) \\\n",
    "      .add(\"Zipcode\",IntegerType(),True) \\\n",
    "      .add(\"ZipCodeType\",StringType(),True) \\\n",
    "      .add(\"City\",StringType(),True) \\\n",
    "      .add(\"State\",StringType(),True) \\\n",
    "      .add(\"LocationType\",StringType(),True) \\\n",
    "      .add(\"Lat\",DoubleType(),True) \\\n",
    "      .add(\"Long\",DoubleType(),True) \\\n",
    "      .add(\"Xaxis\",IntegerType(),True) \\\n",
    "      .add(\"Yaxis\",DoubleType(),True) \\\n",
    "      .add(\"Zaxis\",DoubleType(),True) \\\n",
    "      .add(\"WorldRegion\",StringType(),True) \\\n",
    "      .add(\"Country\",StringType(),True) \\\n",
    "      .add(\"LocationText\",StringType(),True) \\\n",
    "      .add(\"Location\",StringType(),True) \\\n",
    "      .add(\"Decommisioned\",BooleanType(),True) \\\n",
    "      .add(\"TaxReturnsFiled\",StringType(),True) \\\n",
    "      .add(\"EstimatedPopulation\",IntegerType(),True) \\\n",
    "      .add(\"TotalWages\",IntegerType(),True) \\\n",
    "      .add(\"Notes\",StringType(),True)\n",
    "      \n",
    "df_with_schema = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(\"resources/zipcodes.csv\")\n",
    "\n",
    "df_with_schema.printSchema()\n",
    "df_with_schema.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd5b23",
   "metadata": {},
   "source": [
    "### Write PySpark DataFrame to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "905fe358",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/Users/ron/Documents/GitHub/Spark/resources/spark_output/zipcodes already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresources/spark_output/zipcodes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1799\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1782\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1783\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1797\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1798\u001b[0m )\n\u001b[0;32m-> 1799\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/Users/ron/Documents/GitHub/Spark/resources/spark_output/zipcodes already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "df.write.option('header', True).csv('resources/spark_output/zipcodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ef93111",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.options(headers=True, delimiter='~').csv('resources/spark_output/zipcodes1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fae29660",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.mode('overwrite').csv('resources/spark_output/zipcodes1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5197a9",
   "metadata": {},
   "source": [
    "## PySpark Read & Write Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ba10396",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =[(\"James \",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "              (\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "              (\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "              (\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "              (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)]\n",
    "columns=[\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df=spark.createDataFrame(data,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9678b85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('resources/people.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "181da7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parDF=spark.read.parquet('resources/people.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76bc43c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|dob  |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "952a3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode('append').parquet('resources/people.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b57399cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode('overwrite').parquet('resources/people.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bbdd62",
   "metadata": {},
   "source": [
    "### Executing SQL queries on a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36e7b59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|dob  |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parDF.createOrReplaceTempView('ParquetTable')\n",
    "parqSQL = spark.sql('select * from ParquetTable where salary >= 4000')\n",
    "parqSQL.printSchema()\n",
    "parqSQL.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7f3aa",
   "metadata": {},
   "source": [
    "### Create a table from a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e2662c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|dob  |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('CREATE TEMPORARY VIEW PERSON USING parquet OPTIONS (path \\\"resources/people.parquet\")')\n",
    "spark.sql('SELECT * FROM PERSON').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cf321e",
   "metadata": {},
   "source": [
    "### Create parquet partition file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65d30619",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.partitionBy('gender','salary').mode('overwrite').parquet('resources/people2.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a08e441",
   "metadata": {},
   "source": [
    "### Retrieving from a partitioned parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f69d7be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+\n",
      "|firstname|middlename|lastname|dob  |salary|\n",
      "+---------+----------+--------+-----+------+\n",
      "|James    |          |Smith   |36636|3000  |\n",
      "|Michael  |Rose      |        |40288|4000  |\n",
      "|Robert   |          |Williams|42114|4000  |\n",
      "+---------+----------+--------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parDF2=spark.read.parquet('resources/people2.parquet/gender=M')\n",
    "parDF2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4275ef5",
   "metadata": {},
   "source": [
    "### Creating a table on a partitioned parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0df173c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+\n",
      "|firstname|middlename|lastname|dob  |salary|\n",
      "+---------+----------+--------+-----+------+\n",
      "|Maria    |Anne      |Jones   |39192|4000  |\n",
      "|Jen      |Mary      |Brown   |     |-1    |\n",
      "+---------+----------+--------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('create temporary view PERSON2 using parquet options (path \\\"resources/people2.parquet/gender=F\")')\n",
    "spark.sql('select * from PERSON2').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51fc78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
